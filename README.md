# Accelerating Multi-head Attention for Efficient Vision Transformers

This repository releases the code for our final project in EE 451 class at USC.

In this work, you will be able to: 

- Profile and identify the computational bottlenecks in Self-attention (SA) Block
- Implement a serial version of SA using C programming
- Implement a parallel matrix multiplication kernel on a GPU platform using CUDA
- Further optimize the parallel matrix multiplication kernel using shared memory
- Implement an alternative attention kernel called linear attention


## Requirements

- PyTorch
- CUDA

## Quick Start

[TBD]


## Evaluation

Serial Implementation

Parallel Implementation
